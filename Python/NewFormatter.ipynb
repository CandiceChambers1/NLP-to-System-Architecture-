{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Formatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\candi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\candi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\candi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\candi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Words to be preserved for understanding basis in Grammar\n",
    "stopwords_remove = ['from','to','and', 'it', 'with']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Words and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_list = ['Human', 'Acoustic', 'Biologcal', 'Chemical', 'Electrical', 'ElectroMagnetic', 'Hydraulic', 'Magnetic',\n",
    "               'Mechanical', 'Thermal', 'Radioactive', 'Pneumatic']\n",
    "\n",
    "# List of words present in Functional basis\n",
    "separate_list = ['separate', 'divide', 'extract', 'remove', 'isolate', 'sever', 'disjoin', 'detach', 'detaches',\n",
    "                 'release', 'sort', 'split', 'disconnect', 'subtract', 'refine', 'filter', 'purify', 'purifies', 'percolate', 'strain', 'clear', 'cut', 'drill', 'lathe', 'polish',\n",
    "                 'polishes', 'sand']\n",
    "\n",
    "distribute_list = ['distribute', 'diffuse', 'dispel', 'disperse', 'dissipate', 'diverge', 'scatter']\n",
    "\n",
    "import_list = ['import', 'form entrance', 'allow', 'allows','input', 'capture']\n",
    "\n",
    "export_list = ['export', 'dispose', 'eject', 'emit', 'empty', 'remove', 'destroy', 'eliminate']\n",
    "\n",
    "transfer_list = ['transfer', 'transport', 'transmit', 'carry', 'carries', 'deliver', 'advance', 'lift', 'move',\n",
    "                 'conduct', 'convey']\n",
    "guide_list = ['guide', 'translate', 'rotate','direct', 'shift', 'steer', 'straighten', 'switch', 'switches',\n",
    "              'move', 'relocate', 'spin', 'turn', 'constrain',\n",
    "              'unfasten', 'unlock']\n",
    "couple_list = ['couple', 'join', 'link', 'associate', 'assemble', 'fasten', 'attach', 'attaches']\n",
    "\n",
    "\n",
    "\n",
    "mix_list = ['mix', 'mixes', 'add', 'blend', 'coalesce', 'combine', 'pack']\n",
    "\n",
    "actuate_list = ['actuate', 'enable', 'initiate', 'start', 'turn on']\n",
    "\n",
    "regulate_list = ['regulate', 'increase', 'decrease', 'control', 'equalize', 'limit', 'maintain', 'open', 'close',\n",
    "                 'delay', 'interrupt']\n",
    "change_list = ['change', 'increment', 'decrement', 'shape', 'condition', 'adjust', 'modulate', 'demodulate', 'invert',\n",
    "               'normalize', 'rectify',\n",
    "               'rectifies', 'reset', 'scale', 'vary', 'varies', 'modify', 'modifies', 'amplify', 'amplifies', 'enhance',\n",
    "               'magnify', 'magnifies',\n",
    "               'multiply', 'multiplies', 'attenuate', 'dampen', 'reduce', 'compact', 'compress', 'compresses', 'crush',\n",
    "               'crushes', 'pierce', 'deform', 'form', 'prepare', 'adapt', 'treat']\n",
    "\n",
    "stop_list = ['stop', 'prevent', 'inhibit', 'end', 'hault', 'pause', 'interrupt', 'restrain', 'disable', 'turn off',\n",
    "             'shield', 'insulate', 'protect', 'resist']\n",
    "\n",
    "convert_list = ['convert', 'condense', 'create', 'decode', 'differentiate', 'digitize', 'encode', 'evaporate',\n",
    "                'generate', 'integrate', 'liquefy', 'liquifies', 'process', 'solidify',\n",
    "                'solidifies', 'transform']\n",
    "store_list = ['store', 'contain', 'collect', 'accumulate', 'enclose', 'absorb', 'consume', 'fill', 'reserve']\n",
    "\n",
    "supply_list = ['supply', 'supplies', 'provide', 'replenish', 'replenishes', 'retrieve']\n",
    "\n",
    "sense_list = ['sense', 'detect', 'measure', 'feel', 'determine', 'discern', 'perceive', 'recognize', 'identify',\n",
    "              'identifies', 'locate']\n",
    "indicate_list = ['indicate', 'track', 'display', 'announce', 'show', 'denote', 'record', 'register', 'mark', 'time',\n",
    "                 'expose', 'select']\n",
    "process_list = ['process', 'processes', 'calculate', 'check']\n",
    "\n",
    "energize_list = ['energize', 'deenergize']\n",
    "\n",
    "# Different adjectives to be used as a Fuzzy system\n",
    "temperature = ['Cryogenic', 'Frozen', 'Chilled', 'Freezing', 'Cold', 'Cool', 'Nomral Temperature', 'Room Temperature',\n",
    "               'Lukewarm', 'Toasty', 'Mild', 'Warm',\n",
    "               'Heated', 'Hot', 'Cooked', 'Toasted', 'Boiling', 'Burning', 'Steaming']\n",
    "velocity = ['Static', 'Still', 'Creeping', 'Sluggish', 'Slow', 'Flowing', 'Moving', 'Fast', 'Rapid']\n",
    "material = ['Solid', 'Liquid', 'Gas', 'Mixture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_verbs = []\n",
    "all_lists = [separate_list, distribute_list, import_list, export_list, \n",
    "            transfer_list, guide_list, couple_list, mix_list, actuate_list, \n",
    "            regulate_list, change_list, stop_list, convert_list, store_list, \n",
    "            supply_list, sense_list, indicate_list, process_list, \n",
    "            energize_list]\n",
    "\n",
    "allPreserveList = [temperature, functional_verbs, velocity, stopwords_remove, \n",
    "                  energy_list, material]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Characters in Front of the Reserved Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCharacters(List):\n",
    "    new_list = []\n",
    "    for elem in List:\n",
    "        if elem[-1] == \"h\":\n",
    "            new_list.append(elem + 'es')\n",
    "        elif elem[-1] == 'y':\n",
    "            if elem[-2] in ['a', 'e', 'i', 'o', 'u']:\n",
    "                new_list.append(elem + 's')\n",
    "            else:\n",
    "                new_list.append(elem[:-1] + 'ies')\n",
    "        elif elem[-1] == 's':\n",
    "            if elem[-2] == 's':\n",
    "                new_list.append(elem + 'es')\n",
    "            else:\n",
    "                new_list.append(elem + 's')\n",
    "        elif elem[-1] == 'x':\n",
    "            new_list.append(elem + 'es')\n",
    "        else:\n",
    "            new_list.append(elem + 's')\n",
    "    List = []\n",
    "    List.extend(new_list)\n",
    "    \n",
    "    return List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editLists(all_lists):\n",
    "    temporary_list = []\n",
    "    new_all_lists = []\n",
    "    for i in all_lists: \n",
    "        extended_list = addCharacters(i)\n",
    "        temporary_list = extended_list\n",
    "        new_all_lists.append(temporary_list)\n",
    "        temporary_list = []\n",
    "        for i in extended_list:\n",
    "            functional_verbs.append(i)\n",
    "            i = functional_verbs\n",
    "            \n",
    "    return new_all_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perserving words present in temperature, velocity, energy and functional verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perserveWords(allPreserveList):\n",
    "    for j in allPerserveList: \n",
    "        if i.lower() in stop_words:\n",
    "            stop_words.remove(i.lower())\n",
    "\n",
    "#stop_words.add('The','A')\n",
    "# Adding some words to the stop words list that we do not require and removing words we need \n",
    "stopwords_add = ['The', 'A']\n",
    "for i in stopwords_add:\n",
    "    stop_words.add(i)\n",
    "\n",
    "for i in stopwords_remove:\n",
    "    stop_words.remove(i)\n",
    "    \n",
    "ps = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_raw_text(input_paragraph):\n",
    "    try:\n",
    "        words = nltk.word_tokenize(input_paragraph)\n",
    "        return remove_stopwords_from_tokenized_text(words)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "def remove_stopwords_from_tokenized_text(words):\n",
    "    try:\n",
    "        output_list = []\n",
    "\n",
    "        for w in words:\n",
    "            if w not in stop_words:\n",
    "                output_list.append(w)\n",
    "\n",
    "        return output_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consists can be used in the same way as before\n",
    "# Converting words not in nltk synonym list to the ones that are accepted\n",
    "def changeSynonyms(final_list, new_all_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    index = 0\n",
    "    one_list = []\n",
    "    \n",
    "    for i in final_list:\n",
    "        for j in new_all_list:\n",
    "            one_list = j\n",
    "            for k in one_list:\n",
    "                if (i == k):\n",
    "                    if i.lower()[-1] == 's':\n",
    "                        if one_list[0][-1] == 'x':\n",
    "                            final_list[index] = one_list[0] + 'es'\n",
    "                        elif one_list[0][-1] == 'y':\n",
    "                            final_list[index] = one_list[0][:-1] + 'ies'\n",
    "                        else:\n",
    "                            final_list[index] = one_list[0]\n",
    "                    break       \n",
    "        index = index + 1\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createParagraphs(final_list): \n",
    "    new_para = \"\"\n",
    "    for ele in final_list:\n",
    "        if ele == \".\":\n",
    "            new_para = new_para + ele\n",
    "        else:\n",
    "            new_para = new_para + \" \" + ele\n",
    "    return new_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatNouns(new_para):\n",
    "    new_para_latest = \"\"\n",
    "    for i in new_para.split(\".\"):\n",
    "        if \"consists\" in i:\n",
    "            within_words = i.split()\n",
    "            for words in within_words:\n",
    "                if (words == \"and\" or words == \"consists\" or words == \",\"):\n",
    "                    new_para_latest = new_para_latest[:-1] + \" \" + words + \" \"\n",
    "                else:\n",
    "                    new_para_latest = new_para_latest + words.upper() + \"_\"\n",
    "            new_para_latest = new_para_latest[:-1] + \".\\n\"\n",
    "        elif \"connected\" in i:\n",
    "            within_words = i.split()\n",
    "            for words in within_words:\n",
    "                if (words == \"connected\" or words == \"and\" or words == ','):\n",
    "                    new_para_latest = new_para_latest[:-1] + \" \" + words + \" \"\n",
    "                elif (words == \"to\"):\n",
    "                    new_para_latest = new_para_latest + \" \" + words + \" \"\n",
    "                else:\n",
    "                    new_para_latest = new_para_latest + words.upper() + \"_\"\n",
    "            new_para_latest = new_para_latest[:-1] + \".\\n\"\n",
    "        elif (\"imports\" in i) or (\"exports\" in i) or (\"transfers\" in i) or (\"guides\" in i) or (\"supplies\" in i):\n",
    "            within_words = i.split()\n",
    "            index_func = 0\n",
    "            index_from = 0\n",
    "            index_to = 0            \n",
    "            for m in range(len(within_words)):\n",
    "                words = ['imports', 'exports', 'transfers', 'guides', 'supplies']\n",
    "                for i in words:\n",
    "                    if i in within_words:\n",
    "                        occured_word = i\n",
    "                index_func = within_words.index(occured_word)\n",
    "                if (within_words[m] == \"from\"):\n",
    "                    index_from = m\n",
    "                elif (within_words[m] == \"to\"):\n",
    "                    index_to = m\n",
    "                else:\n",
    "                    continue\n",
    "            m = 0\n",
    "            while (m < len(within_words)):\n",
    "                if (index_func > 0 and m == 0):\n",
    "                    for x in range(index_func):\n",
    "                        new_para_latest = new_para_latest + within_words[m].upper() + \"_\"\n",
    "                        m = m + 1\n",
    "                    new_para_latest = new_para_latest[:-1]\n",
    "                elif (index_from > 0 and index_to > 0 and m == index_from):\n",
    "                    new_para_latest = new_para_latest + \" from\" + \" \"\n",
    "                    m = m + 1\n",
    "                    for x in range(index_from + 1, index_to):\n",
    "                        new_para_latest = new_para_latest + within_words[m].upper() + \"_\"\n",
    "                        m = m + 1\n",
    "                    new_para_latest = new_para_latest[:-1] + \" to \"\n",
    "                    m = m + 1\n",
    "                    for x in range(index_to + 1, len(within_words)):\n",
    "                        new_para_latest = new_para_latest + within_words[m].upper() + \"_\"\n",
    "                        m = m + 1\n",
    "                    new_para_latest = new_para_latest[:-1]\n",
    "                                        \n",
    "                elif (index_from > 0 and m == index_from):\n",
    "                    new_para_latest = new_para_latest + \" from \"\n",
    "                    m = m + 1\n",
    "                    for x in range(index_from + 1, len(within_words)):\n",
    "                        new_para_latest = new_para_latest + within_words[m].upper() + \"_\"\n",
    "                        m = m + 1\n",
    "                    new_para_latest = new_para_latest[:-1]\n",
    "                elif (index_to > 0 and m == index_to):\n",
    "                    new_para_latest = new_para_latest + \" to \"\n",
    "                    m = m + 1\n",
    "                    for x in range(index_to + 1, len(within_words)):\n",
    "                        new_para_latest = new_para_latest + within_words[m].upper() + \"_\"\n",
    "                        m = m + 1\n",
    "                    new_para_latest = new_para_latest[:-1]\n",
    "                else:\n",
    "                    new_para_latest = new_para_latest + \" \" + within_words[m]\n",
    "                    m += 1\n",
    "            new_para_latest = new_para_latest + \".\\n\"\n",
    "        \n",
    "        elif ((\"converts\" in i) or (\"mixes\" in i) or (\"couples\" in i) or (\"separates\" in i) or (\"energizes\" in i) or (\n",
    "                \"deenergizes\" in i) or (\"stores\" in i) or (\"stops\" in i) or (\"changes\" in i) or (\"regulates\" in i)):\n",
    "            within_words = i.split()\n",
    "            words = ['converts', 'mixes', 'couples', 'separates', 'energizes', 'deenergizes', 'stores', 'stops', 'changes',\n",
    "                     'regulates']\n",
    "            for i in words:\n",
    "                if i in within_words:\n",
    "                    occured_word = i\n",
    "            index_of_ow = within_words.index(occured_word)\n",
    "            for j in range(len(within_words)):\n",
    "                if j < index_of_ow:\n",
    "                    new_para_latest = new_para_latest + within_words[j].upper() + \"_\"\n",
    "                elif j == index_of_ow:\n",
    "                    new_para_latest = new_para_latest[:-1] + \" \" + within_words[j]\n",
    "                else:\n",
    "                    new_para_latest = new_para_latest + \" \" + within_words[j]\n",
    "            new_para_latest = new_para_latest + \".\\n\"\n",
    "        elif \"distributes\" in i:\n",
    "            within_words = i.split()\n",
    "            index = within_words.index(\"distributes\")\n",
    "            index_to = within_words.index(\"to\")\n",
    "            for x in range(len(within_words)):\n",
    "                if x < index:\n",
    "                    new_para_latest = new_para_latest + within_words[x].upper() + \"_\"\n",
    "                elif x == index:\n",
    "                    new_para_latest = new_para_latest[:-1] + \" \" + within_words[x]\n",
    "                elif x == index_to:\n",
    "                    new_para_latest = new_para_latest + \" \" + within_words[x] + \" \"\n",
    "                else:\n",
    "                    if x > index_to:\n",
    "                        if within_words[x] == \"and\":\n",
    "                            new_para_latest = new_para_latest[:-1] + \" \" + within_words[x] + \" \"\n",
    "                        else:\n",
    "                            new_para_latest = new_para_latest + within_words[x].upper() + \"_\"\n",
    "                    else:\n",
    "                        new_para_latest = new_para_latest + \" \" + within_words[x]\n",
    "            new_para_latest = new_para_latest[:-1] + \".\\n\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return new_para_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_synonyms(word):\n",
    "    var = False\n",
    "    exchange_word = ''\n",
    "    try:\n",
    "        stemmed_word = ps.stem(word)\n",
    "        if ((word == '.') or (word == ',')):\n",
    "            return word\n",
    "        new_word = '\"' + stemmed_word + '\"'\n",
    "        for syn in wordnet.synsets(new_word):\n",
    "            for lm in syn.lemmas():\n",
    "                for x in functional_verbs:\n",
    "                    if (lm.name()).lower() == x.lower():\n",
    "                        var = True;\n",
    "                        exchange_word = x\n",
    "        if var == True:\n",
    "            return exchange_word\n",
    "            var = False\n",
    "        else:\n",
    "            return word\n",
    "    except Exception as e:\n",
    "        word.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_if_in_NLTK(new_para_latest):\n",
    "    new_final_list = []\n",
    "    for item in word_tokenize(new_para_latest):\n",
    "        var = False\n",
    "        for i in energy_list:\n",
    "            if i.lower() == item.lower():\n",
    "                new_final_list.append(i)\n",
    "                var = True\n",
    "                break\n",
    "        if var == True:\n",
    "            continue\n",
    "        else:\n",
    "            for j in temperature:\n",
    "                if j.lower() == item.lower():\n",
    "                    new_final_list.append(j)\n",
    "                    var = True\n",
    "                    break\n",
    "            if var:\n",
    "                continue\n",
    "            else:\n",
    "                for k in velocity:\n",
    "                    if k.lower() == item.lower():\n",
    "                        new_final_list.append(k)\n",
    "                        var = True\n",
    "                        break\n",
    "                if var:\n",
    "                    continue\n",
    "                else:\n",
    "                    for l in material:\n",
    "                        if l.lower() == item.lower():\n",
    "                            new_final_list.append(l)\n",
    "                            var = True\n",
    "                            break\n",
    "                    if var:\n",
    "                        continue\n",
    "                    else:\n",
    "                        new_item = convert_synonyms(item)\n",
    "                        if (new_item == None):\n",
    "                            new_final_list.append(item)\n",
    "                        else:\n",
    "                            new_final_list.append(new_item)\n",
    "\n",
    "                            \n",
    "    return new_final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalVersion(new_final_list):\n",
    "    str = ''\n",
    "    for element in new_final_list:\n",
    "        if element == '.':\n",
    "            str = str + element + \"\\n\"\n",
    "        else:\n",
    "            str = str + \" \" + element\n",
    "\n",
    "    send_to_grammar = \"\"\n",
    "    additional_reserved_words = [',', 'consists', 'connected']\n",
    "    for i in str.split(\".\"):\n",
    "        indexes = []\n",
    "        within_words = i.split()\n",
    "        for j in within_words:\n",
    "            if (j.isupper() or (j in functional_verbs) or (j in temperature) or (j in velocity) or (j in material) or (\n",
    "                    j in additional_reserved_words) or (j in stopwords_remove)):\n",
    "                continue\n",
    "            else:\n",
    "                indexes.append(within_words.index(j))\n",
    "        if len(indexes) >= 2:\n",
    "            for j in range(len(indexes) - 1):\n",
    "                if (indexes[j] + 1 == indexes[j + 1]):\n",
    "                    within_words[indexes[j]] = within_words[indexes[j]][:1].upper() + within_words[indexes[j]][1:]\n",
    "        if (i != \"\\n\"):\n",
    "            for j in within_words:\n",
    "                send_to_grammar = send_to_grammar + j + \" \"\n",
    "            send_to_grammar = send_to_grammar[:-1] + \".\\n\"\n",
    "\n",
    "    return send_to_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_paragraph = \"The FGS System consists of the Left Side FGS, the Right Side FGS, a LR Bus, and a RL Bus. LR Bus is connected to Left Side FGS and Right Side FGS. RL Bus is connected to Left Side FGS and Right Side FGS. The Left Side FGS imports boolean Input from the Left Transfer Switch. The Left Side FGS imports boolean Input from the Left Primary Side. The Right Side FGS imports boolean Input from the Right Transfer Switch. The Right Side FGS imports boolean Input from the Right Primary Side. CLK_ONE supplies synchronous value to Left Side FGS. CLK_TWO supplies synchronous value to LR_Bus. CLK_THREE supplies synchronous value to Right Side FGS. CLK_FOUR supplies synchronous value to RL_Bus.\"\n",
    "input_paragraph = \"The coffeemaker consists of a cooking unit and a pot. The pot is connected to the cooking unit. The cooking unit consists of a tank, a heating unit, and a brewing unit. The tank is connected to the heating unit. The heating unit is connected to the brewing unit. The pot consists of a glassware, a lid, and a handle. The lid is connected to the glassware. The handle is connected to the glassware. The tank imports water and transfers it to the heating unit. The heating unit consists of a heating coil and a hot water pipe. The heating coil is connected to the hot water pipe. The heating coil imports electricity and converts it to heat. The heating coil transfers heat to the hot water pipe. The hot water pipe receives water from the tank. The hot water pipe imports heat from the heating coil. The hot water pipe imports water and heat and energizes it to hot water. The brewing unit consists of a vertical pipe, a water valve, a shower head, a filter, and a filter holder. The lower end of the vertical pipe is connected to the hot water pipe. The water valve is connected to the vertical pipe. The shower head is connected to the upper end of the vertical pipe. The filer holder consists of a filter. The vertical pipe receives hot water from the hot water pipe. The vertical pipe transfers hot water to the shower head. The shower head receives hot water from the vertical pipe. The shower head distributes it to filter. The filter imports ground coffee. The filter receives hot water from the shower head. The filter couples ground coffee and hot water. The pot stores the liquid coffee.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COFFEEMAKER consists COOKING_UNIT and POT.\n",
      "POT connected to COOKING_UNIT.\n",
      "COOKING_UNIT consists TANK , HEATING_UNIT , and BREWING_UNIT.\n",
      "TANK connected to HEATING_UNIT.\n",
      "HEATING_UNIT connected to BREWING_UNIT.\n",
      "POT consists GLASSWARE , LID , and HANDLE.\n",
      "LID connected to GLASSWARE.\n",
      "HANDLE connected to GLASSWARE.\n",
      "TANK_IMPORTS_WATER_AND transfers it to HEATING_UNIT.\n",
      "HEATING_UNIT consists HEATING_COIL and HOT_WATER_PIPE.\n",
      "HEATING_COIL connected to HOT_WATER_PIPE.\n",
      "HEATING_COIL imports electricity and converts it to HEAT.\n",
      "HEATING_COIL transfers heat to HOT_WATER_PIPE.\n",
      "HOT_WATER_PIPE imports heat from HEATING_COIL.\n",
      "HOT_WATER_PIPE imports water and heat and energizes it to HOT_WATER.\n",
      "BREWING_UNIT consists VERTICAL_PIPE , WATER_VALVE , SHOWER_HEAD , FILTER , and FILTER_HOLDER.\n",
      "LOWER_END_VERTICAL_PIPE connected to HOT_WATER_PIPE.\n",
      "WATER_VALVE connected to VERTICAL_PIPE.\n",
      "SHOWER_HEAD connected to UPPER_END_VERTICAL_PIPE.\n",
      "FILER_HOLDER consists FILTER.\n",
      "VERTICAL_PIPE transfers Hot water to SHOWER_HEAD.\n",
      "SHOWER_HEAD distributes it to FILTER.\n",
      "FILTER imports Ground coffee.\n",
      "FILTER couples Ground coffee and Hot water.\n",
      "POT stores Liquid coffee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Driver Code\n",
    "if __name__ == \"__main__\":\n",
    "    functional_verbs = []\n",
    "    final_list = []\n",
    "    new_para = ''\n",
    "    new_para_latest = ''\n",
    "    new_final_list = []\n",
    "    send_to_grammar = ''\n",
    "    new_all_list = []\n",
    "    \n",
    "    # Edit all individual list and update the all_lists\n",
    "    new_all_list = editLists(all_lists)\n",
    "    \n",
    "    # Remove stopwords from raw text \n",
    "    final_list = remove_stopwords_from_raw_text(input_paragraph)\n",
    "    \n",
    "    # Change synonyms to our preferred  words\n",
    "    changeSynonyms(final_list, new_all_list)\n",
    "    \n",
    "    # Convert the list to a paragraph \n",
    "    new_para = createParagraphs(final_list)\n",
    "    \n",
    "    # Reformat the nouns for ANTLR standards \n",
    "    new_para_latest = formatNouns(new_para)\n",
    "    \n",
    "    # \n",
    "    new_final_list = convert_if_in_NLTK(new_para_latest)\n",
    "    send_to_grammar = finalVersion(new_final_list)\n",
    "    \n",
    "    print(send_to_grammar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
