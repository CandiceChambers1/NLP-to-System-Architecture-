{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS investigation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Investigative POS Tagging (nltk vs spaCy)"
      ],
      "metadata": {
        "id": "xn0cjIuOT0-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloads"
      ],
      "metadata": {
        "id": "bEauQvzZUKCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABHy8se7UMZM",
        "outputId": "186d1875-121c-458d-b8cc-1fe0878fd0a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Using cached spacy-3.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Using cached spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Using cached pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Using cached thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Using cached spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Using cached catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Using cached typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Using cached srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Collecting pathy>=0.3.5\n",
            "  Using cached pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.3 spacy-legacy-3.0.9 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.15 typer-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install plantuml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "581MWXStf-sx",
        "outputId": "3c5a780a-a53e-4b5c-9a5a-55753d13e734"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plantuml in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.7/dist-packages (from plantuml) (0.17.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "pLi2tjj3UEUr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WXC1fh4sT0RF"
      },
      "outputs": [],
      "source": [
        "# Spacy Imports\n",
        "import spacy \n",
        "from spacy.lang.en import English\n",
        "\n",
        "# TextBlob Imports \n",
        "from textblob import TextBlob\n",
        "\n",
        "# nltk Imports \n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "#plantUML Imports \n",
        "import plantuml\n",
        "from plantuml import PlantUML\n",
        "\n",
        "# Other Imports \n",
        "import string\n",
        "from os.path import abspath"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nltk downloads"
      ],
      "metadata": {
        "id": "WKx1IxK_Uhmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cqj1Uf2UQ_l",
        "outputId": "2d595eeb-7120-4175-d4f7-738c3883634c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qIGpXf6UPGK",
        "outputId": "d5d986e1-c0d8-4a06-9609-b1aa0b810ead"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text POS Tags \n",
        "Investigating the POS Tagging Attribute"
      ],
      "metadata": {
        "id": "eCn7rPwKUok4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" 2.\tLR_Bus establishes connection between Left_Side_FGS and Right_Side_FGS.\"\n",
        "text = nltk.word_tokenize(text)\n",
        "l1 = nltk.pos_tag(text)\n",
        "l1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46XFfvnzUngI",
        "outputId": "8b2564e2-7db2-462a-cd10-271736b7e234"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('2', 'CD'),\n",
              " ('.', '.'),\n",
              " ('LR_Bus', 'NNP'),\n",
              " ('establishes', 'VBZ'),\n",
              " ('connection', 'NN'),\n",
              " ('between', 'IN'),\n",
              " ('Left_Side_FGS', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('Right_Side_FGS', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Noun Extraction (only extracting Proper Nouns)\n",
        "\n",
        "nltk_nouns = []\n",
        "for index,tuple in enumerate(l1):\n",
        "  if tuple[1] == 'NNP':\n",
        "    nltk_nouns.append(tuple[0])\n",
        "\n",
        "nltk_nouns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IL9IKaoVOQi",
        "outputId": "6387db03-ba5a-4e1d-c465-cce4cb04ff3a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LR_Bus', 'Left_Side_FGS', 'Right_Side_FGS']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating text file to store info to be runned in PlantUML "
      ],
      "metadata": {
        "id": "8CVua2WeZCv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specification text\n",
        "text = (\" 1.\tThe FGS_System consists of four components: the Left_Side_FGS, the Right_Side_FGS, an LR_Bus, and an RL_Bus.\"\n",
        "        \" 2.\tLR_Bus establishes connection between Left_Side_FGS and Right_Side_FGS.\"\n",
        "        \" 3.\tRL_Bus establishes connection between Left_Side_FGS and Right_Side_FGS.\"\n",
        "        \" 4.\tThe Left_Side_FGS accepts as input a boolean value of Left_Transfer_Switch and Left_Primary_Side.\"\n",
        "        \" 5.\tThe Right_Side_FGS accepts as input a boolean value of Right_Transfer_Switch and Right_Primary_Side.\"\n",
        "        \" 6.\tThe Left_Side_FGS takes input from a synchronous clock CLK1.\"\n",
        "        \" 7.\tThe LR_Bus takes input from CLK2.\"\n",
        "        \" 8.\tThe Right_Side_FGS takes input from a synchronous clock CLK3.\"\n",
        "        \" 9.\tThe RL_Bus takes input from CLK4.\")\n"
      ],
      "metadata": {
        "id": "MEh4mI_MZCPW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from nltk.tokenize import sent_tokenize, word_tokenize\n",
        " sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2zZT7TRdJDT",
        "outputId": "2239d053-571f-44ca-8595-878ae09040b4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' 1.',\n",
              " 'The FGS_System consists of four components: the Left_Side_FGS, the Right_Side_FGS, an LR_Bus, and an RL_Bus.',\n",
              " '2.',\n",
              " 'LR_Bus establishes connection between Left_Side_FGS and Right_Side_FGS.',\n",
              " '3.',\n",
              " 'RL_Bus establishes connection between Left_Side_FGS and Right_Side_FGS.',\n",
              " '4.',\n",
              " 'The Left_Side_FGS accepts as input a boolean value of Left_Transfer_Switch and Left_Primary_Side.',\n",
              " '5.',\n",
              " 'The Right_Side_FGS accepts as input a boolean value of Right_Transfer_Switch and Right_Primary_Side.',\n",
              " '6.',\n",
              " 'The Left_Side_FGS takes input from a synchronous clock CLK1.',\n",
              " '7.',\n",
              " 'The LR_Bus takes input from CLK2.',\n",
              " '8.',\n",
              " 'The Right_Side_FGS takes input from a synchronous clock CLK3.',\n",
              " '9.',\n",
              " 'The RL_Bus takes input from CLK4.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Editing or creating text file if it doesn't exists\n",
        "# Creating the code for plantuml\n",
        "\n",
        "# Used to tokenize a sentence \n",
        "doc = sent_tokenize(text)\n",
        "\n",
        "connection_num = 1\n",
        "with open('model_specs.txt', 'w') as f:\n",
        "    with open('model_specs.txt','a') as f: \n",
        "      f.write(\"skinparam nodesep 150\\n\")\n",
        "    # Iterate sentence-by-sentence then word-by-word \n",
        "    for sent in doc:\n",
        "\n",
        "      # Tokenize that sentence \n",
        "      token_sentence = []\n",
        "      token_sentence = nltk.word_tokenize(sent)\n",
        "\n",
        "      # POS Tag the tokens \n",
        "      pos_tag_token = nltk.pos_tag(token_sentence)\n",
        "\n",
        "\n",
        "      # Noun Extraction (only extracting Proper Nouns)\n",
        "\n",
        "      nltk_nouns = []\n",
        "      for index,tuple in enumerate(pos_tag_token):\n",
        "        if tuple[1] == 'NNP':\n",
        "          nltk_nouns.append(tuple[0])\n",
        "\n",
        "      # print(nltk_nouns)\n",
        "    \n",
        "      i = 1\n",
        "      j = 1\n",
        "      for word in token_sentence: \n",
        "          if i < (len(nltk_nouns)):  \n",
        "            if \"connection\" in token_sentence:\n",
        "              # print(f\"{nltk_nouns[0]}-{nltk_nouns[i]} :C{connection_num}\")\n",
        "              with open('model_specs.txt','a') as f: \n",
        "                f.write(f\"[{nltk_nouns[0]}]-[{nltk_nouns[i]}] :C{connection_num}\\n\")\n",
        "              \n",
        "              i = i+1\n",
        "              connection_num = connection_num+1\n",
        "          else: \n",
        "            break\n",
        "          \n",
        "          if j < (len(nltk_nouns)):  \n",
        "            if \"input\" in token_sentence: \n",
        "              # print(f\"{nltk_nouns[0]}-{nltk_nouns[j]}\")\n",
        "              with open('model_specs.txt', 'a') as f:     \n",
        "                f.write(f\"[{nltk_nouns[0]}]-{nltk_nouns[j]}\\n\")\n",
        "              j = j+1\n",
        "              connection_num = connection_num + 1\n",
        "          else: \n",
        "            break\n",
        "f.close()"
      ],
      "metadata": {
        "id": "e5oM6etwZc9A"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating System Diagram using plantUML"
      ],
      "metadata": {
        "id": "CGAHEhspolLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "server = PlantUML(url='http://www.plantuml.com/plantuml/img/',\n",
        "                          basic_auth={},\n",
        "                          form_auth={}, http_opts={}, request_opts={})\n",
        "\n",
        "server.processes_file(abspath('model_specs.txt'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOGDCvPyifYz",
        "outputId": "48ef4b75-bcd1-4b70-c47e-a011137b9bd5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}