{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Script for NLP to SA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#NLP to Systems Architecture "
      ],
      "metadata": {
        "id": "wI3l26dGOxcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Imports"
      ],
      "metadata": {
        "id": "ZEm6z8_eabfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spacy Imports\n",
        "import spacy \n",
        "from spacy.lang.en import English\n",
        "\n",
        "# TextBlob Imports \n",
        "from textblob import TextBlob\n",
        "\n",
        "# nltk Imports \n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "# Other Imports \n",
        "import string"
      ],
      "metadata": {
        "id": "NNxGfMMWairn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Downloads"
      ],
      "metadata": {
        "id": "89Yf79RaO5oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5GIE2r-sMBH",
        "outputId": "8073f26f-62fa-4002-9128-a1162ba695f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating text file to store info to be runned in PlantUML "
      ],
      "metadata": {
        "id": "g5TwYqwyG_b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specification text\n",
        "text = (\" 1.\tThe FGS_System consists of four components: the Left_Side FGS, the Right_Side FGS, an LR_Bus, and an RL_Bus.\"\n",
        "        \" 2.\tLR_Bus establishes connection between Left_Side FGS and Right_Side FGS.\"\n",
        "        \" 3.\tRL_Bus establishes connection between Left_Side FGS and Right_Side FGS.\"\n",
        "        \" 4.\tThe Left_Side FGS accepts as input a boolean value of Left_Transfer_Switch and Left_Primary_Side.\"\n",
        "        \" 5.\tThe Right_Side FGS accepts as input a boolean value of Right_Transfer_Switch and Right_Primary_Side.\"\n",
        "        \" 6.\tThe Left_Side FGS takes input from a synchronous clock CLK1.\"\n",
        "        \" 7.\tThe LR_Bus takes input from CLK2.\"\n",
        "        \" 8.\tThe Right_Side FGS takes input from a synchronous clock CLK3.\"\n",
        "        \" 9.\tThe RL_Bus takes input from CLK4.\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n"
      ],
      "metadata": {
        "id": "6FriMZDZuR1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the nouns using spcay \n",
        "tb_noun_phrases = TextBlob(text).noun_phrases\n",
        "tb_noun_phrases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6T9CuX433s1",
        "outputId": "a4d71ae4-ff59-4006-efd2-08f3929fc9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['fgs_system', 'left_side fgs', 'right_side fgs', 'lr_bus', 'rl_bus', 'lr_bus', 'establishes connection', 'left_side fgs', 'right_side fgs', 'rl_bus', 'establishes connection', 'left_side fgs', 'right_side fgs', 'left_side fgs', 'boolean value', 'left_transfer_switch', 'left_primary_side', 'right_side fgs', 'boolean value', 'right_transfer_switch', 'right_primary_side', 'left_side fgs', 'synchronous clock', 'clk1', 'lr_bus', 'clk2', 'right_side fgs', 'synchronous clock', 'clk3', 'rl_bus', 'clk4'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the nouns using spcay \n",
        "# Must run nlp = spacy.load(\"en_core_web_sm\")\n",
        "#          doc = nlp(text)\n",
        "sp_noun_phrases =[chunk.text for chunk in doc.noun_chunks]\n",
        "print(sp_noun_phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md9vvdYq2qBq",
        "outputId": "cb014e87-5200-4294-aa1d-f24f42bfac58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The FGS_System', 'four components', 'the Left_Side FGS', 'the Right_Side FGS', 'an LR_Bus', 'an RL_Bus.', 'establishes connection', 'Left_Side FGS', 'Right_Side FGS', 'RL_Bus', 'establishes connection', 'Left_Side FGS', 'Right_Side FGS', 'The Left_Side FGS', 'input', 'a boolean value', 'Left_Transfer_Switch', 'Left_Primary_Side', 'The Right_Side FGS', 'input', 'a boolean value', 'Right_Transfer_Switch', 'The Left_Side FGS', 'input', 'The LR_Bus', 'input', 'CLK2', 'The Right_Side FGS', 'input', 'a synchronous clock CLK3', 'The RL_Bus', 'input', 'CLK4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence segmenation \n",
        "nlp = English()  # just the language with no pipeline\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_GWqmhLs1SF",
        "outputId": "43b26c5e-3911-4a86-c6db-1f5071567448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1.\n",
            "\tThe FGS_System consists of four components: the Left_Side FGS, the Right_Side FGS, an LR_Bus, and an RL_Bus.\n",
            "2.\n",
            "\tLR_Bus establishes connection between Left_Side FGS and Right_Side FGS.\n",
            "3.\n",
            "\tRL_Bus establishes connection between Left_Side FGS and Right_Side FGS.\n",
            "4.\n",
            "\tThe Left_Side FGS accepts as input a boolean value of Left_Transfer_Switch and Left_Primary_Side.\n",
            "5.\n",
            "\tThe Right_Side FGS accepts as input a boolean value of Right_Transfer_Switch and Right_Primary_Side.\n",
            "6.\n",
            "\tThe Left_Side FGS takes input from a synchronous clock CLK1.\n",
            "7.\n",
            "\tThe LR_Bus takes input from CLK2.\n",
            "8.\n",
            "\tThe Right_Side FGS takes input from a synchronous clock CLK3.\n",
            "9.\n",
            "\tThe RL_Bus takes input from CLK4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words \n",
        "stopwords_english = stopwords.words('english')\n",
        "stopwords_english.append(\"\\t\")\n",
        "stopwords_english.append(\" \")"
      ],
      "metadata": {
        "id": "0Gancbo7sz-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Editing or creating text file if it doesn't exists\n",
        "# Creating the code for plantuml\n",
        "\n",
        "# Used to tokenize a sentence \n",
        "nlp = English()  # just the language with no pipeline\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "doc = nlp(text)\n",
        "\n",
        "connection_num = 1\n",
        "with open('/content/docs/model_specs.txt', 'w') as f:\n",
        "    # Iterate sentence-by-sentence then word-by-word \n",
        "    for sent in doc.sents:\n",
        "\n",
        "      # Extract the nouns from each sentences\n",
        "      # Components are mentioned before it sub-component(s) \n",
        "      tb_noun_phrases = TextBlob(str(sent)).noun_phrases\n",
        "      # print(tb_noun_phrases)\n",
        "\n",
        "      # Tokenize that sentence \n",
        "      token_sentence = []\n",
        "      for token in sent:\n",
        "        token_sentence.append(token.lower_)\n",
        "\n",
        "      # Clean that sentence \n",
        "      sentence_clean = []\n",
        "      for word in token_sentence: # Go through every word in your tokens list\n",
        "        if (word not in stopwords_english and # remove stopwords\n",
        "            word not in string.punctuation and \n",
        "            word not in string.digits):  # remove punctuation\n",
        "            sentence_clean.append(word)\n",
        "      # print(sentence_clean)\n",
        "      i = 2\n",
        "      j = 2\n",
        "      for word in sentence_clean: \n",
        "        if i < (len(tb_noun_phrases)):  \n",
        "          if \"connection\" in sentence_clean:\n",
        "            # print(f\"{tb_noun_phrases[0]}-{tb_noun_phrases[i]} :C{connection_num}\")\n",
        "            with open('/content/docs/model_specs.txt','a') as f: \n",
        "              f.write(f\"{tb_noun_phrases[0]}-{tb_noun_phrases[i]} :C{connection_num}\\n\")\n",
        "            i = i+1\n",
        "            connection_num = connection_num+1\n",
        "        else: \n",
        "          break\n",
        "        \n",
        "        if j < (len(tb_noun_phrases)):  \n",
        "          if \"input\" in sentence_clean: \n",
        "            # print(f\"{tb_noun_phrases[0]}-{tb_noun_phrases[j]}\")\n",
        "            with open('/content/docs/model_specs.txt', 'a') as f:     \n",
        "              f.write(f\"{tb_noun_phrases[0]}-{tb_noun_phrases[j]}\\n\")\n",
        "            j = j+1\n",
        "            connection_num = connection_num + 1\n",
        "        else: \n",
        "          break\n",
        "f.close()"
      ],
      "metadata": {
        "id": "9vKaVctyG-3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate further !!!!!\n",
        "word = 'accepts input'\n",
        "word = nltk.word_tokenize(word)\n",
        "l1 = nltk.pos_tag(word)\n",
        "l1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGK0aYBLBxWr",
        "outputId": "eb2c1829-5ac7-416d-dd40-b6e8b7ee2bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('accepts', 'NNS'), ('input', 'VBP')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}